# 🏗️ 项目架构总览

## 项目简介

NYLG（Network Information Crawler）是一个基于Flask + Vue.js的现代化网络信息爬取系统，采用前后端分离架构，支持多种网站的数据采集和实时下载功能。

## 🎯 核心特性

- **多平台爬虫支持**：百度搜索、B站视频、CSDN文章、自定义URL
- **实时通信**：基于WebSocket的实时任务状态推送
- **现代化前端**：Vue 3 + Element Plus响应式界面
- **模块化设计**：清晰的分层架构，易于扩展和维护
- **智能内容识别**：自动识别网页中的文本、图片、视频等内容

## 🏛️ 系统架构
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   前端层 (Vue)   │    │   API层 (Flask) │    │   业务层         │
│                 │    │                 │    │                 │
│ • Vue 3         │◄──►│ • RESTful API   │◄──►│ • 爬虫服务       │
│ • Element Plus  │    │ • WebSocket     │    │ • 认证服务       │
│ • Axios         │    │ • 路由管理        │    │ • 下载服务       │
└─────────────────┘    └─────────────────┘    └─────────────────┘
▲
│
┌─────────────────┐
│   数据层         │
│                 │
│ • 爬虫引擎       │
│ • 状态管理       │
│ • 文件存储       │
└─────────────────┘


## 📂 目录结构详解

### 核心模块

- **`api/`** - API接口层，处理HTTP请求和响应
- **`services/`** - 业务逻辑层，封装核心功能
- **`spiders/`** - 爬虫引擎层，实现各平台数据采集
- **`core/`** - 核心状态管理
- **`frontend/`** - 前端资源和模板
- **`utils/`** - 通用工具函数

### 数据流向

1. **用户交互** → 前端Vue组件
2. **API请求** → Flask路由处理
3. **业务处理** → 服务层调用
4. **数据采集** → 爬虫引擎执行
5. **结果返回** → WebSocket实时推送
6. **状态更新** → 前端界面刷新

## 🔧 技术栈

### 后端技术
- **Flask** - 轻量级Web框架
- **Flask-SocketIO** - WebSocket实时通信
- **Requests** - HTTP请求库
- **BeautifulSoup** - HTML解析
- **Pillow** - 图像处理

### 前端技术
- **Vue 3** - 渐进式JavaScript框架
- **Element Plus** - Vue 3组件库
- **Vite** - 现代化构建工具
- **Socket.IO** - 实时通信客户端
- **Axios** - HTTP客户端

## 🚀 启动流程

1. **环境初始化** - 虚拟环境和依赖安装
2. **服务启动** - Flask应用和SocketIO服务
3. **前端构建** - Vue项目编译和静态资源生成
4. **路由注册** - API端点和WebSocket事件绑定
5. **状态初始化** - 全局变量和任务队列准备

## 📊 性能特点

- **异步处理**：支持并发爬取任务
- **智能延迟**：随机延迟避免反爬检测
- **错误恢复**：完善的异常处理机制
- **资源优化**：合理的内存和网络使用

---

> 💡 **设计理念**：本项目采用现代化的微服务思想，通过清晰的分层架构实现高内聚低耦合，确保系统的可维护性和可扩展性。

---

# 🕷️ 爬虫引擎深度解析

## 概述

本项目的爬虫引擎采用模块化设计，每个爬虫类独立实现特定平台的数据采集逻辑。通过统一的接口规范，实现了可插拔的爬虫架构。

## 🏗️ 爬虫架构设计

### 统一接口规范

所有爬虫类都遵循相同的接口设计：

```python
class SpiderInterface:
    def search(self, keyword: str, max_results: int = 10, **kwargs) -> List[Dict]:
        """统一的搜索接口"""
        pass
```

### 核心组件

1. **请求管理器** - 处理HTTP请求和响应
2. **解析引擎** - 提取和清洗数据
3. **反爬策略** - 模拟真实用户行为
4. **错误处理** - 异常捕获和恢复

## 🔍 百度搜索爬虫 (BaiduSpider)

### 技术原理

百度搜索爬虫通过模拟浏览器行为，解析搜索结果页面的HTML结构来提取数据。

```python
class BaiduSpider:
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',
        # 多个User-Agent轮换使用
    ]
    
    def search(self, keyword: str, max_results: int = 10, search_type: str = 'web'):
        # 构建搜索URL
        if search_type == 'web':
            url = f'https://www.baidu.com/s?wd={quote(keyword)}'
        elif search_type == 'image':
            url = f'https://image.baidu.com/search/index?tn=baiduimage&word={quote(keyword)}'
```

### 关键技术点

#### 1. 反爬虫策略
```python
def _get_soup(self, url: str):
    # 随机延迟
    time.sleep(random.uniform(*self.delay_range))
    
    # 随机User-Agent
    headers = {**self.headers, 'User-Agent': random.choice(self.USER_AGENTS)}
    
    # 设置Referer
    headers['Referer'] = 'https://www.baidu.com/'
```

#### 2. 数据解析
```python
def _search_web(self, url: str, max_results: int):
    soup = self._get_soup(url)
    results = []
    
    # 解析搜索结果容器
    result_containers = soup.find_all('div', class_='result')
    
    for container in result_containers[:max_results]:
        # 提取标题
        title_elem = container.find('h3') or container.find('a')
        title = title_elem.get_text(strip=True) if title_elem else '无标题'
        
        # 提取链接
        link_elem = container.find('a', href=True)
        url = link_elem['href'] if link_elem else ''
        
        # 提取描述
        desc_elem = container.find('span', class_='content-right_8Zs40')
        description = desc_elem.get_text(strip=True) if desc_elem else ''
```

### 数据结构

返回的数据结构统一为：
```python
{
    'index': 1,
    'title': '搜索结果标题',
    'url': 'https://example.com',
    'description': '搜索结果描述',
    'image_url': 'https://image.url'  # 仅图片搜索
}
```

## 📺 B站视频爬虫 (BilibiliSpider)

### 技术特点

B站爬虫专门针对视频内容进行优化，能够提取视频的详细元数据。

#### 1. 视频信息提取
```python
def _parse_video_item(self, item):
    # 提取视频标题
    title_elem = item.find('a', {'title': True}) or item.find('h3', {'title': True})
    title = title_elem.get('title', '').strip() if title_elem else ''
    
    # 提取视频链接
    link_elem = item.find('a', href=True)
    if link_elem:
        href = link_elem['href']
        if href.startswith('//'):
            url = 'https:' + href
        elif href.startswith('/'):
            url = 'https://www.bilibili.com' + href
    
    # 提取UP主信息
    author_elem = item.find('a', class_='up-name') or item.find('span', class_='up-name')
    author = author_elem.get_text(strip=True) if author_elem else '未知UP主'
    
    # 提取播放量
    play_elem = item.find('span', class_='so-icon watch-num')
    play_count = play_elem.get_text(strip=True) if play_elem else '0'
```

#### 2. 反爬检测处理
```python
def _get_soup(self, url: str):
    response = requests.get(url, headers=self.headers, timeout=10)
    
    # 检测反爬机制
    if len(response.text) < 5000 or "验证" in response.text:
        print("⚠️ 可能触发了反爬机制")
        return None
```

## 📝 CSDN文章爬虫 (CSDNSpider)

### API接口调用

CSDN爬虫采用API接口方式，直接获取JSON数据，避免了HTML解析的复杂性。

```python
def search(self, keyword: str, max_results: int = 10):
    # 构建API请求URL
    url = f'https://so.csdn.net/api/v3/search?q={quote(keyword)}&t=all&p=1&s=0&tm=0&lv=-1&ft=0&l=&u=&ct=-1&pnt=-1&ry=-1&ss=-1&dct=-1&vco=-1'
    
    json_data = self._get_json(url)
    if not json_data or 'result_vos' not in json_data:
        return []
```

### 数据清洗
```python
def _clean_html_tags(self, text: str):
    """清除HTML标签"""
    if not text:
        return text
    return re.sub(r'<[^>]+>', '', text)

def _parse_article(self, article: dict):
    return {
        'index': len(results) + 1,
        'title': self._clean_html_tags(article.get('title', '')),
        'url': article.get('url', ''),
        'description': self._clean_html_tags(article.get('description', '')),
        'author': article.get('nickname', ''),
        'publish_time': article.get('create_time', '')
    }
```

## 🌐 URL内容提取器 (WebContentExtractor)

### 智能内容识别

URL提取器是最复杂的组件，需要处理各种类型的网页内容。

#### 1. 结构化内容提取
```python
def get_web_content(self, url: str):
    resp = requests.get(url, headers=self.headers, timeout=15)
    resp.encoding = resp.apparent_encoding or resp.encoding or 'utf-8'
    
    html_content = resp.text
    
    # 提取结构化信息
    title = self.extract_page_title(html_content)
    text_content = self.extract_text_content(html_content)
    images = self.extract_images(html_content, url)
    
    # 构建结构化内容
    structured_content = {
        'title': title,
        'url': url,
        'text_length': len(text_content),
        'images_count': len(images),
        'preview': text_content[:500] + '...' if len(text_content) > 500 else text_content,
        'full_content': text_content,
        'images': images[:10],
        'metadata': {
            'content_type': resp.headers.get('content-type', ''),
            'status_code': resp.status_code,
            'final_url': resp.url,
            'charset': resp.encoding or 'utf-8'
        }
    }
```

#### 2. 文本内容提取
```python
def extract_text_content(self, html: str) -> str:
    # 移除脚本和样式标签
    html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
    
    # 移除HTML标签
    text = re.sub(r'<[^>]+>', '', html)
    
    # 清理空白字符
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text
```

#### 3. 图片链接提取
```python
def extract_images(self, html: str, base_url: str) -> list:
    img_pattern = r'<img[^>]+src=["\']([^"\'>]+)["\'][^>]*>'
    img_matches = re.findall(img_pattern, html, re.IGNORECASE)
    
    images = []
    for img_url in img_matches:
        # 处理相对URL
        full_url = urljoin(base_url, img_url)
        
        # 验证图片URL
        if self._is_valid_image_url(full_url):
            images.append(full_url)
    
    return list(set(images))  # 去重
```

## 🛡️ 反爬虫策略

### 1. 请求头伪装
- 随机User-Agent轮换
- 设置合理的Referer
- 模拟真实浏览器请求头

### 2. 访问频率控制
- 随机延迟机制
- 请求间隔控制
- 并发数量限制

### 3. 错误处理
- 网络超时重试
- HTTP状态码检查
- 反爬检测识别

## 📊 性能优化

### 1. 内存管理
- 及时释放大对象
- 限制结果数量
- 流式处理大文件

### 2. 网络优化
- 连接池复用
- 压缩传输
- 超时控制

### 3. 缓存策略
- 结果缓存
- 请求去重
- 智能更新

---

> 🔧 **技术要点**：爬虫引擎的核心在于平衡效率与稳定性，通过合理的反爬策略和错误处理机制，确保长期稳定运行。

---

# 🌐 前后端架构详解

## 架构概述

本项目采用现代化的前后端分离架构，通过RESTful API和WebSocket实现数据交互，提供了良好的用户体验和系统可维护性。

## 🔧 后端架构 (Flask)

### 应用工厂模式

```python
# src/app.py
def create_app():
    app = Flask(__name__)
    
    # 配置CORS
    CORS(app, resources={r"/api/*": {"origins": "*"}})
    
    # 初始化SocketIO
    socketio.init_app(app, cors_allowed_origins="*")
    
    # 注册蓝图
    from src.api.crawl import api_bp
    from src.api.bilibili import bili_bp
    from src.api.stats import stats_bp
    
    app.register_blueprint(api_bp)
    app.register_blueprint(bili_bp)
    app.register_blueprint(stats_bp)
    
    return app
```

### 蓝图模块化设计

#### 1. 爬虫API蓝图 (`api/crawl.py`)

**核心接口**：
- `GET /api/spiders` - 获取可用爬虫列表
- `POST /api/crawl` - 执行爬取任务
- `GET /api/task/{task_id}` - 查询任务状态

**请求处理流程**：
```python
@api_bp.route('/api/crawl', methods=['POST'])
def crawl():
    # 1. 参数验证
    data = request.get_json() or {}
    spider_type = data.get('spider_type')
    if not spider_type:
        return jsonify({'success': False, 'message': '缺少爬虫类型'}), 400
    
    # 2. 任务创建
    task_counter += 1
    task = {
        'id': task_counter,
        'spider_type': spider_type,
        'status': 'running',
        'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    # 3. 爬虫执行
    if task_type == 'search':
        search_results, error = crawler_service.execute_search(
            spider_type, keyword, max_results, data_type
        )
    elif task_type == 'url':
        content_data, error = crawler_service.extract_url_content(url)
    
    # 4. 结果返回
    return jsonify({
        'success': True, 
        'message': '爬取任务完成', 
        'task_id': task_counter, 
        'results': result['data']
    })
```

#### 2. B站功能蓝图 (`api/bilibili.py`)

**认证相关**：
- `GET /api/bilibili/login/qr` - 获取登录二维码
- `POST /api/bilibili/login/poll` - 轮询登录状态
- `GET /api/bilibili/login/status` - 检查登录状态
- `POST /api/bilibili/logout` - 用户登出

**视频功能**：
- `POST /api/bilibili/video/info` - 获取视频信息
- `POST /api/bilibili/video/download` - 下载视频
- `GET /api/bilibili/video/download/status/{task_id}` - 查询下载状态

**二维码登录实现**：
```python
@bili_bp.get('/api/bilibili/login/qr')
def get_bilibili_qr():
    ok, msg, data = auth.get_qr()
    if not ok:
        return jsonify({'success': False, 'message': msg})
    return jsonify({'success': True, **data})

# 认证服务实现
def get_qr():
    # 1. 获取二维码数据
    resp = login_session.get(
        'https://passport.bilibili.com/x/passport-login/web/qrcode/generate?source=main-fe-header'
    ).json()
    
    # 2. 生成二维码图片
    login_url = resp['data']['url']
    qr = qrcode.QRCode(version=1, box_size=10, border=5)
    qr.add_data(login_url)
    qr.make(fit=True)
    img = qr.make_image(fill_color="black", back_color="white")
    
    # 3. 转换为Base64
    buffer = io.BytesIO()
    img.save(buffer, format='PNG')
    img_str = base64.b64encode(buffer.getvalue()).decode()
    qr_data_url = f"data:image/png;base64,{img_str}"
    
    return True, "", {
        'qr_url': qr_data_url,
        'login_url': login_url,
        'qrcode_key': resp['data']['qrcode_key']
    }
```

### 服务层设计

#### 爬虫服务 (`services/crawler_service.py`)

```python
class CrawlerService:
    def __init__(self):
        self.spiders = {
            'baidu': {'name': '百度搜索', 'class': BaiduSpider, 'type': 'search'},
            'bilibili': {'name': '哔哩哔哩', 'class': BilibiliSpider, 'type': 'search'},
            'csdn': {'name': 'CSDN', 'class': CSDNSpider, 'type': 'search'},
            'url': {'name': '自定义URL', 'class': WebContentExtractor, 'type': 'url'}
        }
    
    def execute_search(self, spider_type: str, keyword: str, max_results: int, data_type: str):
        """执行搜索任务"""
        if spider_type not in self.spiders:
            return None, f"不支持的爬虫类型: {spider_type}"
        
        spider_class = self.spiders[spider_type]['class']
        spider = spider_class()
        
        try:
            if spider_type == 'baidu':
                results = spider.search(keyword, max_results, data_type)
            else:
                results = spider.search(keyword, max_results)
            
            return results, None
        except Exception as e:
            return None, str(e)
```

### WebSocket实时通信

```python
# src/extensions.py
from flask_socketio import SocketIO
socketio = SocketIO(cors_allowed_origins="*")

# 下载进度推送
def emit_download_progress(task_id, progress_data):
    socketio.emit('download_progress', {
        'task_id': task_id,
        'progress': progress_data['progress'],
        'speed': progress_data['speed'],
        'eta': progress_data['eta']
    })
```

## 🎨 前端架构 (Vue 3)

### 项目结构
frontend/vue/
├── src/
│   ├── components/          # 可复用组件
│   ├── views/              # 页面组件
│   │   ├── Crawler.vue     # 爬虫主界面
│   │   ├── Docs.vue        # 文档页面
│   │   └── About.vue       # 关于页面
│   ├── router/             # 路由配置
│   ├── style/              # 样式文件
│   ├── App.vue             # 根组件
│   └── main.js             # 入口文件
├── public/                 # 静态资源
└── package.json            # 依赖配置


### 主应用入口

```javascript
// src/main.js
import { createApp } from 'vue'
import App from './App.vue'
import router from './router'
import ElementPlus from 'element-plus'
import 'element-plus/dist/index.css'
import * as ElementPlusIconsVue from '@element-plus/icons-vue'

const app = createApp(App)

// 注册Element Plus图标
for (const [key, component] of Object.entries(ElementPlusIconsVue)) {
  app.component(key, component)
}

app.use(router)
app.use(ElementPlus)
app.mount('#app')
```

### 核心组件设计

#### 爬虫主界面 (`views/Crawler.vue`)

**组件结构**：
```vue
<template>
  <div class="crawler-container">
    <!-- 爬虫类型选择 -->
    <el-card class="spider-selector">
      <el-radio-group v-model="selectedSpider">
        <el-radio-button 
          v-for="spider in spiders" 
          :key="spider.id" 
          :label="spider.id"
        >
          {{ spider.icon }} {{ spider.name }}
        </el-radio-button>
      </el-radio-group>
    </el-card>
    
    <!-- 参数配置 -->
    <el-card class="params-config">
      <el-form :model="currentParams" label-width="120px">
        <el-form-item label="搜索关键词" v-if="isSearchType">
          <el-input v-model="currentParams.keyword" placeholder="请输入搜索关键词" />
        </el-form-item>
        <el-form-item label="目标URL" v-if="isUrlType">
          <el-input v-model="currentParams.url" placeholder="请输入要爬取的URL" />
        </el-form-item>
        <el-form-item label="结果数量">
          <el-input-number v-model="currentParams.max_results" :min="1" :max="100" />
        </el-form-item>
      </el-form>
    </el-card>
    
    <!-- 执行按钮 -->
    <el-button 
      type="primary" 
      size="large" 
      :loading="isExecuting" 
      :disabled="!canExecute"
      @click="executeCrawl"
    >
      {{ isExecuting ? '爬取中...' : '开始爬取' }}
    </el-button>
    
    <!-- 结果展示 -->
    <el-card class="results-display" v-if="results.length > 0">
      <template #header>
        <div class="results-header">
          <span>爬取结果 ({{ results.length }}条)</span>
          <el-button type="success" @click="exportResults">导出结果</el-button>
        </div>
      </template>
      
      <div class="results-list">
        <div 
          v-for="(result, index) in paginatedResults" 
          :key="index" 
          class="result-item"
        >
          <h4>{{ result.title }}</h4>
          <p class="result-url">{{ result.url }}</p>
          <p class="result-description">{{ result.description }}</p>
          <div class="result-actions">
            <el-button size="small" @click="toggleResultDetail(index)">查看详情</el-button>
          </div>
        </div>
      </div>
      
      <!-- 分页 -->
      <el-pagination
        v-model:current-page="currentPage"
        :page-size="pageSize"
        :total="results.length"
        layout="prev, pager, next, jumper"
        @current-change="handlePageChange"
      />
    </el-card>
  </div>
</template>
```

**响应式数据管理**：
```javascript
<script setup>
import { ref, computed, watch, onMounted } from 'vue'
import axios from 'axios'
import { io } from 'socket.io-client'

// 响应式数据
const selectedSpider = ref('baidu')
const isExecuting = ref(false)
const results = ref([])
const spiders = ref([])
const currentPage = ref(1)
const pageSize = ref(10)

// 爬虫参数
const baiduParams = ref({ keyword: '', max_results: 20, data_type: 'web' })
const bilibiliParams = ref({ keyword: '', max_results: 20 })
const csdnParams = ref({ keyword: '', max_results: 20 })
const urlParams = ref({ url: '' })

// 计算属性
const canExecute = computed(() => {
  if (isExecuting.value) return false
  
  const spider = spiders.value.find(s => s.id === selectedSpider.value)
  if (!spider) return false
  
  if (spider.type === 'search') {
    return currentKeyword.value.trim().length > 0
  } else if (spider.type === 'url') {
    return urlParams.value.url.trim().length > 0
  }
  
  return false
})

const currentKeyword = computed(() => {
  switch (selectedSpider.value) {
    case 'baidu': return baiduParams.value.keyword
    case 'bilibili': return bilibiliParams.value.keyword
    case 'csdn': return csdnParams.value.keyword
    default: return ''
  }
})

const paginatedResults = computed(() => {
  const start = (currentPage.value - 1) * pageSize.value
  const end = start + pageSize.value
  return results.value.slice(start, end)
})

// 方法
const executeCrawl = async () => {
  isExecuting.value = true
  results.value = []
  
  try {
    const spider = spiders.value.find(s => s.id === selectedSpider.value)
    let requestData = { spider_type: selectedSpider.value }
    
    if (spider.type === 'search') {
      requestData.keyword = currentKeyword.value
      requestData.max_results = getCurrentParams().max_results
      if (selectedSpider.value === 'baidu') {
        requestData.data_type = baiduParams.value.data_type
      }
    } else if (spider.type === 'url') {
      requestData.url = urlParams.value.url
    }
    
    const response = await axios.post('/api/crawl', requestData)
    
    if (response.data.success) {
      results.value = response.data.results || []
      ElMessage.success(`爬取完成，获得 ${results.value.length} 条结果`)
    } else {
      ElMessage.error(response.data.message || '爬取失败')
    }
  } catch (error) {
    console.error('爬取错误:', error)
    ElMessage.error('网络错误，请稍后重试')
  } finally {
    isExecuting.value = false
  }
}

const loadSpiders = async () => {
  try {
    const response = await axios.get('/api/spiders')
    spiders.value = response.data
  } catch (error) {
    console.error('加载爬虫列表失败:', error)
  }
}

// 生命周期
onMounted(() => {
  loadSpiders()
})
</script>
```

### Socket.IO实时通信

```javascript
// WebSocket连接管理
const socket = ref(null)
const downloadTasks = ref([])

const initSocket = () => {
  socket.value = io()
  
  // 监听下载进度
  socket.value.on('download_progress', (data) => {
    const task = downloadTasks.value.find(t => t.id === data.task_id)
    if (task) {
      task.progress = data.progress
      task.speed = data.speed
      task.eta = data.eta
    }
  })
  
  // 监听下载完成
  socket.value.on('download_complete', (data) => {
    const task = downloadTasks.value.find(t => t.id === data.task_id)
    if (task) {
      task.status = 'completed'
      task.file_path = data.file_path
      ElMessage.success(`下载完成: ${task.title}`)
    }
  })
  
  // 监听下载错误
  socket.value.on('download_error', (data) => {
    const task = downloadTasks.value.find(t => t.id === data.task_id)
    if (task) {
      task.status = 'error'
      task.error = data.error
      ElMessage.error(`下载失败: ${data.error}`)
    }
  })
}

const disconnectSocket = () => {
  if (socket.value) {
    socket.value.disconnect()
    socket.value = null
  }
}
```

## 🔄 数据流转机制

### 请求-响应流程

1. **用户操作** → 前端Vue组件触发事件
2. **参数验证** → 前端验证输入参数有效性
3. **API请求** → Axios发送HTTP请求到Flask后端
4. **路由分发** → Flask根据URL路由到对应的蓝图处理函数
5. **业务处理** → 服务层调用爬虫引擎执行任务
6. **数据返回** → JSON格式响应返回给前端
7. **界面更新** → Vue响应式系统更新界面显示

### WebSocket实时通信流程

1. **连接建立** → 前端建立Socket.IO连接
2. **任务启动** → 后端启动异步下载任务
3. **进度推送** → 后端定期推送任务进度
4. **前端更新** → 前端实时更新进度条和状态
5. **任务完成** → 后端推送完成通知
6. **结果展示** → 前端显示最终结果

## 🛡️ 安全性设计

### CORS配置
```python
CORS(app, resources={r"/api/*": {"origins": "*"}})
```

### 输入验证
```python
def crawl():
    data = request.get_json() or {}
    spider_type = data.get('spider_type')
    if not spider_type:
        return jsonify({'success': False, 'message': '缺少爬虫类型'}), 400
```

### 错误处理
```javascript
try {
  const response = await axios.post('/api/crawl', requestData)
  // 处理成功响应
} catch (error) {
  console.error('爬取错误:', error)
  ElMessage.error('网络错误，请稍后重试')
}
```

---

> 🚀 **架构优势**：前后端分离架构提供了良好的可维护性和扩展性，WebSocket实时通信增强了用户体验，模块化设计便于功能扩展和维护。

---

# ⚙️ 核心功能实现详解

## 概述

本文档深入解析NYLG爬虫系统的核心功能实现，包括B站认证系统、视频下载功能、状态管理机制等关键技术点。

## 🔐 B站认证系统

### 二维码登录机制

B站采用二维码扫码登录方式，整个流程包括：二维码生成 → 用户扫码 → 状态轮询 → 获取Cookie

#### 1. 二维码生成

```python
# src/services/bilibili_auth.py
def get_qr():
    global login_session
    try:
        login_session = requests.session()
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...'
        }
        
        # 请求B站二维码接口
        resp = login_session.get(
            'https://passport.bilibili.com/x/passport-login/web/qrcode/generate?source=main-fe-header',
            headers=headers
        ).json()
        
        if resp['code'] != 0:
            return False, "获取二维码失败", None
        
        # 生成二维码图片
        login_url = resp['data']['url']
        qr = qrcode.QRCode(version=1, box_size=10, border=5)
        qr.add_data(login_url)
        qr.make(fit=True)
        img = qr.make_image(fill_color="black", back_color="white")
        img = img.resize((200, 200), Image.LANCZOS)
        
        # 转换为Base64编码
        buffer = io.BytesIO()
        img.save(buffer, format='PNG')
        img_str = base64.b64encode(buffer.getvalue()).decode()
        qr_data_url = f"data:image/png;base64,{img_str}"
        
        return True, "", {
            'qr_url': qr_data_url,
            'login_url': login_url,
            'qrcode_key': resp['data']['qrcode_key']
        }
    except Exception as e:
        return False, f'获取二维码失败: {str(e)}', None
```

**技术要点**：
- 使用`qrcode`库生成二维码图片
- 通过`PIL`调整图片尺寸
- Base64编码便于前端显示
- Session保持登录状态

#### 2. 登录状态轮询

```python
def poll(qrcode_key: str):
    global global_cookies, global_bili_jct, login_session
    try:
        if not qrcode_key or not login_session:
            return False, '参数错误', None
        
        # 轮询登录状态
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...'
        }
        
        resp = login_session.get(
            f'https://passport.bilibili.com/x/passport-login/web/qrcode/poll?qrcode_key={qrcode_key}',
            headers=headers
        ).json()
        
        if resp['code'] != 0:
            return False, resp.get('message', '轮询失败'), None
        
        data = resp['data']
        
        # 处理不同的登录状态
        if data['code'] == 86101:  # 未扫码
            return False, '请扫描二维码', {'status': 'waiting'}
        elif data['code'] == 86090:  # 已扫码未确认
            return False, '请在手机上确认登录', {'status': 'scanned'}
        elif data['code'] == 0:  # 登录成功
            # 提取Cookie和csrf token
            url = data['url']
            cookie_match = re.search(r'bili_jct=([^&]+)', url)
            if cookie_match:
                global_bili_jct = cookie_match.group(1)
            
            # 保存完整Cookie
            cookies = login_session.cookies
            global_cookies = '; '.join([f'{c.name}={c.value}' for c in cookies])
            
            return True, '登录成功', {
                'status': 'success',
                'cookies': global_cookies,
                'bili_jct': global_bili_jct
            }
        else:
            return False, f'登录失败: {data.get("message", "未知错误")}', None
            
    except Exception as e:
        return False, f'轮询失败: {str(e)}', None
```

**状态码说明**：
- `86101`: 未扫码状态
- `86090`: 已扫码待确认
- `0`: 登录成功
- 其他: 登录失败或过期

### Cookie管理

```python
# 全局变量存储认证信息
global_cookies = ""
global_bili_jct = ""
login_session = None

def status():
    """检查登录状态"""
    global global_cookies, global_bili_jct
    try:
        if global_cookies and global_bili_jct:
            return True, '已登录', {
                'logged_in': True,
                'cookies': global_cookies,
                'bili_jct': global_bili_jct
            }
        else:
            return True, '未登录', {'logged_in': False}
    except Exception as e:
        return False, f'状态检查失败: {str(e)}', None

def logout():
    """用户登出"""
    global global_cookies, global_bili_jct, login_session
    try:
        global_cookies = ""
        global_bili_jct = ""
        login_session = None
        return True, '已登出'
    except Exception as e:
        return False, f'注销失败: {str(e)}'
```

## 📹 视频下载系统

### 视频信息获取

```python
# src/services/bilibili_download.py
def get_video_info(bvid: str, cookies: str = ""):
    """获取视频详细信息"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',
            'Referer': 'https://www.bilibili.com/',
            'Cookie': cookies
        }
        
        # 获取视频基本信息
        info_url = f'https://api.bilibili.com/x/web-interface/view?bvid={bvid}'
        resp = requests.get(info_url, headers=headers)
        data = resp.json()
        
        if data['code'] != 0:
            return None, f"获取视频信息失败: {data.get('message', '未知错误')}"
        
        video_data = data['data']
        
        # 获取视频流信息
        cid = video_data['cid']
        stream_url = f'https://api.bilibili.com/x/player/playurl?bvid={bvid}&cid={cid}&qn=80&fnval=16'
        stream_resp = requests.get(stream_url, headers=headers)
        stream_data = stream_resp.json()
        
        if stream_data['code'] != 0:
            return None, f"获取视频流失败: {stream_data.get('message', '未知错误')}"
        
        # 解析视频流信息
        dash = stream_data['data']['dash']
        video_streams = dash.get('video', [])
        audio_streams = dash.get('audio', [])
        
        # 构建返回数据
        video_info = {
            'bvid': bvid,
            'title': video_data['title'],
            'desc': video_data['desc'],
            'duration': video_data['duration'],
            'owner': video_data['owner']['name'],
            'view': video_data['stat']['view'],
            'danmaku': video_data['stat']['danmaku'],
            'reply': video_data['stat']['reply'],
            'favorite': video_data['stat']['favorite'],
            'coin': video_data['stat']['coin'],
            'share': video_data['stat']['share'],
            'like': video_data['stat']['like'],
            'pic': video_data['pic'],
            'video_streams': [],
            'audio_streams': []
        }
        
        # 处理视频流
        for stream in video_streams:
            video_info['video_streams'].append({
                'quality': stream['id'],
                'quality_desc': get_quality_desc(stream['id']),
                'url': stream['baseUrl'],
                'size': get_stream_file_size(stream['baseUrl'], headers),
                'codecs': stream['codecs']
            })
        
        # 处理音频流
        for stream in audio_streams:
            video_info['audio_streams'].append({
                'quality': stream['id'],
                'url': stream['baseUrl'],
                'size': get_stream_file_size(stream['baseUrl'], headers),
                'codecs': stream['codecs']
            })
        
        return video_info, None
        
    except Exception as e:
        return None, f"获取视频信息异常: {str(e)}"
```

### 异步下载实现

```python
def download_video_async(task_id: str, video_url: str, audio_url: str, 
                         output_path: str, filename: str, headers: dict):
    """异步下载视频和音频"""
    try:
        # 更新任务状态
        if task_id in download_tasks:
            download_tasks[task_id]['status'] = 'downloading'
            download_tasks[task_id]['progress'] = 0
        
        # 创建输出目录
        os.makedirs(output_path, exist_ok=True)
        
        video_path = os.path.join(output_path, f"{filename}_video.mp4")
        audio_path = os.path.join(output_path, f"{filename}_audio.mp4")
        final_path = os.path.join(output_path, f"{filename}.mp4")
        
        # 下载视频流
        success = download_stream_with_progress(
            video_url, video_path, headers, task_id, 'video'
        )
        if not success:
            raise Exception("视频下载失败")
        
        # 下载音频流
        success = download_stream_with_progress(
            audio_url, audio_path, headers, task_id, 'audio'
        )
        if not success:
            raise Exception("音频下载失败")
        
        # 合并视频和音频
        merge_success = merge_video_audio(video_path, audio_path, final_path)
        if not merge_success:
            raise Exception("视频合并失败")
        
        # 清理临时文件
        try:
            os.remove(video_path)
            os.remove(audio_path)
        except:
            pass
        
        # 更新任务状态
        if task_id in download_tasks:
            download_tasks[task_id]['status'] = 'completed'
            download_tasks[task_id]['progress'] = 100
            download_tasks[task_id]['file_path'] = final_path
        
        # 发送完成通知
        socketio.emit('download_complete', {
            'task_id': task_id,
            'file_path': final_path
        })
        
    except Exception as e:
        # 更新错误状态
        if task_id in download_tasks:
            download_tasks[task_id]['status'] = 'error'
            download_tasks[task_id]['error'] = str(e)
        
        # 发送错误通知
        socketio.emit('download_error', {
            'task_id': task_id,
            'error': str(e)
        })

def download_stream_with_progress(url: str, output_path: str, headers: dict, 
                                 task_id: str, stream_type: str):
    """带进度的流下载"""
    try:
        response = requests.get(url, headers=headers, stream=True)
        response.raise_for_status()
        
        total_size = int(response.headers.get('content-length', 0))
        downloaded_size = 0
        
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
                    
                    # 计算进度
                    if total_size > 0:
                        progress = (downloaded_size / total_size) * 100
                        
                        # 发送进度更新
                        socketio.emit('download_progress', {
                            'task_id': task_id,
                            'stream_type': stream_type,
                            'progress': progress,
                            'downloaded': format_file_size(downloaded_size),
                            'total': format_file_size(total_size)
                        })
        
        return True
        
    except Exception as e:
        print(f"下载{stream_type}失败: {e}")
        return False
```

### 视频音频合并

```python
def merge_video_audio(video_path: str, audio_path: str, output_path: str):
    """使用FFmpeg合并视频和音频"""
    try:
        import subprocess
        
        # FFmpeg命令
        cmd = [
            'ffmpeg',
            '-i', video_path,
            '-i', audio_path,
            '-c:v', 'copy',
            '-c:a', 'copy',
            '-y',  # 覆盖输出文件
            output_path
        ]
        
        # 执行合并
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            return True
        else:
            print(f"FFmpeg错误: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"合并失败: {e}")
        return False
```

## 📊 状态管理系统

### 全局状态定义

```python
# src/core/state.py
# 全局任务与结果，与原 app.py 中的全局变量保持一致
tasks = []          # 爬虫任务列表
results = []        # 爬虫结果列表
task_counter = 0    # 任务计数器

# B站下载任务字典
download_tasks = {}  # {task_id: {status, progress, file_path, error, ...}}
```

### 任务状态流转

```python
# 任务状态定义
TASK_STATUS = {
    'pending': '等待中',
    'running': '执行中', 
    'completed': '已完成',
    'failed': '失败',
    'cancelled': '已取消'
}

# 下载任务状态
DOWNLOAD_STATUS = {
    'waiting': '等待下载',
    'downloading': '下载中',
    'merging': '合并中',
    'completed': '下载完成',
    'error': '下载失败'
}
```

### 任务管理API

```python
# src/api/stats.py
@stats_bp.route('/api/tasks')
def get_tasks():
    """获取所有任务列表"""
    return jsonify({
        'success': True,
        'tasks': tasks,
        'total': len(tasks)
    })

@stats_bp.route('/api/task/<int:task_id>')
def get_task(task_id):
    """获取特定任务详情"""
    task = next((t for t in tasks if t['id'] == task_id), None)
    if task:
        return jsonify({'success': True, 'task': task})
    else:
        return jsonify({'success': False, 'message': '任务不存在'}), 404

@stats_bp.route('/api/results')
def get_results():
    """获取所有爬取结果"""
    return jsonify({
        'success': True,
        'results': results,
        'total': len(results)
    })
```

## 🔄 WebSocket事件系统

### 事件定义

```python
# 下载相关事件
SOCKET_EVENTS = {
    'download_progress': '下载进度更新',
    'download_complete': '下载完成',
    'download_error': '下载错误',
    'task_status_change': '任务状态变更',
    'system_notification': '系统通知'
}
```

### 事件发送

```python
from src.extensions import socketio

def emit_download_progress(task_id: str, progress_data: dict):
    """发送下载进度事件"""
    socketio.emit('download_progress', {
        'task_id': task_id,
        'progress': progress_data['progress'],
        'speed': progress_data.get('speed', ''),
        'eta': progress_data.get('eta', ''),
        'downloaded': progress_data.get('downloaded', ''),
        'total': progress_data.get('total', '')
    })

def emit_task_complete(task_id: str, result_data: dict):
    """发送任务完成事件"""
    socketio.emit('download_complete', {
        'task_id': task_id,
        'file_path': result_data.get('file_path', ''),
        'file_size': result_data.get('file_size', ''),
        'duration': result_data.get('duration', '')
    })

def emit_system_notification(message: str, type: str = 'info'):
    """发送系统通知"""
    socketio.emit('system_notification', {
        'message': message,
        'type': type,  # info, success, warning, error
        'timestamp': datetime.now().isoformat()
    })
```

## 🛠️ 工具函数库

### 文件大小格式化

```python
def format_file_size(size_bytes: int) -> str:
    """格式化文件大小显示"""
    if size_bytes == 0:
        return "0 B"
    
    size_names = ["B", "KB", "MB", "GB", "TB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    
    return f"{size_bytes:.2f} {size_names[i]}"

def get_stream_file_size(url: str, headers: dict) -> str:
    """获取流文件大小"""
    try:
        response = requests.head(url, headers=headers, timeout=10)
        content_length = response.headers.get('content-length')
        if content_length:
            return format_file_size(int(content_length))
        return "未知大小"
    except:
        return "未知大小"
```

### 视频质量映射

```python
def get_quality_desc(quality_id: int) -> str:
    """获取视频质量描述"""
    quality_map = {
        120: "4K 超清",
        116: "1080P60 高清",
        112: "1080P+ 高清", 
        80: "1080P 高清",
        74: "720P60 高清",
        64: "720P 高清",
        32: "480P 清晰",
        16: "360P 流畅"
    }
    return quality_map.get(quality_id, f"质量{quality_id}")
```

## 🔍 错误处理机制

### 统一错误处理

```python
class CrawlerError(Exception):
    """爬虫基础异常类"""
    def __init__(self, message: str, error_code: str = None):
        self.message = message
        self.error_code = error_code
        super().__init__(self.message)

class NetworkError(CrawlerError):
    """网络请求异常"""
    pass

class AuthError(CrawlerError):
    """认证异常"""
    pass

class ParseError(CrawlerError):
    """解析异常"""
    pass

def handle_api_error(func):
    """API错误处理装饰器"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except CrawlerError as e:
            return jsonify({
                'success': False,
                'message': e.message,
                'error_code': e.error_code
            }), 400
        except Exception as e:
            return jsonify({
                'success': False,
                'message': f'系统错误: {str(e)}',
                'error_code': 'SYSTEM_ERROR'
            }), 500
    return wrapper
```

### 重试机制

```python
import time
from functools import wraps

def retry(max_attempts: int = 3, delay: float = 1.0, backoff: float = 2.0):
    """重试装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            attempts = 0
            current_delay = delay
            
            while attempts < max_attempts:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    attempts += 1
                    if attempts >= max_attempts:
                        raise e
                    
                    print(f"第{attempts}次尝试失败: {e}, {current_delay}秒后重试")
                    time.sleep(current_delay)
                    current_delay *= backoff
            
            return None
        return wrapper
    return decorator

# 使用示例
@retry(max_attempts=3, delay=1.0)
def fetch_video_info(bvid: str):
    # 可能失败的网络请求
    pass
```

## 📈 性能优化策略

### 1. 连接池管理

```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class OptimizedSession:
    """优化的HTTP会话管理"""
    
    def __init__(self):
        self.session = requests.Session()
        
        # 配置重试策略
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        
        # 配置适配器
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=20,
            pool_maxsize=20
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def get(self, url: str, **kwargs):
        return self.session.get(url, **kwargs)
    
    def post(self, url: str, **kwargs):
        return self.session.post(url, **kwargs)
```

### 2. 缓存机制

```python
import json
import hashlib
from datetime import datetime, timedelta

class SimpleCache:
    """简单内存缓存"""
    
    def __init__(self, default_ttl: int = 300):
        self.cache = {}
        self.default_ttl = default_ttl
    
    def _generate_key(self, data: str) -> str:
        return hashlib.md5(data.encode()).hexdigest()
    
    def get(self, key: str):
        cache_key = self._generate_key(key)
        if cache_key in self.cache:
            item = self.cache[cache_key]
            if datetime.now() < item['expires']:
                return item['data']
            else:
                del self.cache[cache_key]
        return None
    
    def set(self, key: str, data, ttl: int = None):
        cache_key = self._generate_key(key)
        expires = datetime.now() + timedelta(seconds=ttl or self.default_ttl)
        self.cache[cache_key] = {
            'data': data,
            'expires': expires
        }
    
    def clear(self):
        self.cache.clear()

# 全局缓存实例
video_info_cache = SimpleCache(ttl=600)  # 10分钟缓存
```

### 3. 异步任务队列

```python
import threading
from queue import Queue
from concurrent.futures import ThreadPoolExecutor

class TaskQueue:
    """异步任务队列管理"""
    
    def __init__(self, max_workers: int = 5):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.tasks = {}
    
    def submit_task(self, task_id: str, func, *args, **kwargs):
        """提交异步任务"""
        future = self.executor.submit(func, *args, **kwargs)
        self.tasks[task_id] = {
            'future': future,
            'status': 'running',
            'start_time': datetime.now()
        }
        return task_id
    
    def get_task_status(self, task_id: str):
        """获取任务状态"""
        if task_id not in self.tasks:
            return None
        
        task = self.tasks[task_id]
        future = task['future']
        
        if future.done():
            if future.exception():
                task['status'] = 'failed'
                task['error'] = str(future.exception())
            else:
                task['status'] = 'completed'
                task['result'] = future.result()
        
        return task
    
    def cancel_task(self, task_id: str):
        """取消任务"""
        if task_id in self.tasks:
            future = self.tasks[task_id]['future']
            if future.cancel():
                self.tasks[task_id]['status'] = 'cancelled'
                return True
        return False

# 全局任务队列
task_queue = TaskQueue(max_workers=10)
```

## 🔒 安全机制

### 1. 请求频率限制

```python
import time
from collections import defaultdict

class RateLimiter:
    """请求频率限制器"""
    
    def __init__(self, max_requests: int = 60, time_window: int = 60):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = defaultdict(list)
    
    def is_allowed(self, identifier: str) -> bool:
        now = time.time()
        requests = self.requests[identifier]
        
        # 清理过期请求
        requests[:] = [req_time for req_time in requests 
                      if now - req_time < self.time_window]
        
        # 检查是否超过限制
        if len(requests) >= self.max_requests:
            return False
        
        # 记录新请求
        requests.append(now)
        return True
    
    def get_reset_time(self, identifier: str) -> float:
        requests = self.requests[identifier]
        if not requests:
            return 0
        return requests[0] + self.time_window

# 全局限流器
rate_limiter = RateLimiter(max_requests=30, time_window=60)
```

### 2. 输入验证

```python
import re
from urllib.parse import urlparse

def validate_bvid(bvid: str) -> bool:
    """验证B站视频ID格式"""
    if not bvid:
        return False
    return bool(re.match(r'^BV[a-zA-Z0-9]{10}$', bvid))

def validate_url(url: str) -> bool:
    """验证URL格式"""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False

def sanitize_filename(filename: str) -> str:
    """清理文件名中的非法字符"""
    # 移除或替换非法字符
    illegal_chars = r'[<>:"/\\|?*]'
    filename = re.sub(illegal_chars, '_', filename)
    
    # 限制长度
    if len(filename) > 200:
        filename = filename[:200]
    
    return filename.strip()
```

## 📝 总结

本文档详细解析了NYLG爬虫系统的核心功能实现，包括：

1. **认证系统**: B站二维码登录的完整流程
2. **下载系统**: 异步视频下载和合并机制
3. **状态管理**: 全局状态和任务管理
4. **实时通信**: WebSocket事件系统
5. **工具函数**: 文件处理和格式化工具
6. **错误处理**: 统一异常处理和重试机制
7. **性能优化**: 缓存、连接池、任务队列
8. **安全机制**: 频率限制和输入验证

这些核心功能共同构成了一个稳定、高效、安全的网络爬虫系统。通过模块化设计和完善的错误处理，系统具备了良好的可维护性和扩展性。

---

*💡 提示：在实际部署时，建议根据具体需求调整各项参数，如缓存时间、重试次数、并发数量等。